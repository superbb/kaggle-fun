{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8cd2f1ed",
   "metadata": {
    "papermill": {
     "duration": 0.006619,
     "end_time": "2025-06-30T16:34:58.058409",
     "exception": false,
     "start_time": "2025-06-30T16:34:58.051790",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# House Prices Prediction Notebook\n",
    "This notebook predicts house prices for the Kaggle \"Home Data for ML Course\" competition using XGBoost. \n",
    "\n",
    "It was originally a working draft but has been cleaned up with help from Grok (from xAI) into a modular, readable pipeline. \n",
    "\n",
    "It scores 13937 with the setup below. Ranking 136 out of 5580 (as of 2025-06-30)\n",
    "\n",
    " - Data Prep: Loads train/test data, imputes missing values (mostly medians for numeric, 'None' for categorical, but some exceptions for modes and 0s).\n",
    " - Feature Engineering: Adds derived features (TotalSF, HouseAge, TotalBath etc) and neighborhood stats (Neighborhood_MedianPrice, Neighborhood_Qual_MedianPrice, PricePerSqFt).\n",
    " - Encoding: One-hot encodes categoricals with a 1% frequency filter to reduce noise.\n",
    " - Feature Selection: Drops low-importance features (~80% retention) using XGBoost importances.\n",
    "   - Top features are: Neighborhood_Qual_MedianPrice 43%, TotalSF 8%, BsmtQual_Ex 3%)\n",
    " - Tuning: RandomizedSearchCV with 50 iterations, 5-fold CV, and broad XGBoost params (max_depth etc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e1e0e0",
   "metadata": {
    "papermill": {
     "duration": 0.004513,
     "end_time": "2025-06-30T16:34:58.067856",
     "exception": false,
     "start_time": "2025-06-30T16:34:58.063343",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Data Prep\n",
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9720aad6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-30T16:34:58.079193Z",
     "iopub.status.busy": "2025-06-30T16:34:58.078815Z",
     "iopub.status.idle": "2025-06-30T16:35:01.426363Z",
     "shell.execute_reply": "2025-06-30T16:35:01.425119Z"
    },
    "papermill": {
     "duration": 3.356474,
     "end_time": "2025-06-30T16:35:01.428940",
     "exception": false,
     "start_time": "2025-06-30T16:34:58.072466",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Toggle for log transforming the target variable\n",
    "USE_LOG_TRANSFORM = False  # Set to True to enable log transform, False to disable\n",
    "EVAL_METRIC = 'mae'  # Evaluation metric: 'mae', 'rmse', 'mape', etc.\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, RandomizedSearchCV\n",
    "\n",
    "# Global metric mapping\n",
    "# Maps EVAL_METRIC to: (function to compute the metric, sklearn CV scoring string, display name)\n",
    "# - Function: Computes error (e.g., mean_absolute_error for MAE) between predictions and true values.\n",
    "# - CV Scoring: Negative version (e.g., 'neg_mean_absolute_error') for sklearnâ€™s cross_val_score, which maximizes scores (we minimize errors).\n",
    "# - Name: Readable label (e.g., 'MAE') for debug outputs.\n",
    "METRIC_MAP = {\n",
    "    'mae': (mean_absolute_error, 'neg_mean_absolute_error', 'MAE'),\n",
    "    'rmse': (lambda y_true, y_pred: mean_squared_error(y_true, y_pred, squared=False), 'neg_root_mean_squared_error', 'RMSE')\n",
    "}\n",
    "\n",
    "\n",
    "def load_and_prepare_data(train_path, test_path, target_col='SalePrice', debug=False):\n",
    "    \"\"\"\n",
    "    Load and prepare training and test data for modeling.\n",
    "    \n",
    "    Args:\n",
    "        train_path (str): Path to training CSV file.\n",
    "        test_path (str): Path to test CSV file.\n",
    "        target_col (str): Name of the target column (default: 'SalePrice').\n",
    "        debug (bool): If True, print debug information (default: False).\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (X_train, y_train, X_test) - Training features, target, and test features.\n",
    "    \"\"\"\n",
    "    # Load data\n",
    "    X_train = pd.read_csv(train_path, index_col='Id')\n",
    "    X_test = pd.read_csv(test_path, index_col='Id')\n",
    "    \n",
    "    # Separate target and drop rows with missing target\n",
    "    X_train = X_train.dropna(subset=[target_col])\n",
    "    y_train = X_train[target_col]\n",
    "    if USE_LOG_TRANSFORM:\n",
    "        y_train = np.log1p(y_train)  # Apply log transform if enabled\n",
    "        if debug:\n",
    "            print(\"Log transform enabled for target\")\n",
    "\n",
    "    X_train = X_train.drop(columns=[target_col])\n",
    "    \n",
    "    if debug:\n",
    "        print(\"Training data shape:\", X_train.shape)\n",
    "        print(\"Test data shape:\", X_test.shape)\n",
    "        print(\"Target summary:\\n\", y_train.describe())\n",
    "    \n",
    "    return X_train, y_train, X_test\n",
    "\n",
    "# Usage\n",
    "train_path = '/kaggle/input/home-data-for-ml-course/train.csv'\n",
    "test_path = '/kaggle/input/home-data-for-ml-course/test.csv'\n",
    "X_train, y_train, X_test = load_and_prepare_data(train_path, test_path, debug=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95720bd6",
   "metadata": {
    "papermill": {
     "duration": 0.004446,
     "end_time": "2025-06-30T16:35:01.438192",
     "exception": false,
     "start_time": "2025-06-30T16:35:01.433746",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Impute Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "698c1adf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-30T16:35:01.449420Z",
     "iopub.status.busy": "2025-06-30T16:35:01.449015Z",
     "iopub.status.idle": "2025-06-30T16:35:01.527559Z",
     "shell.execute_reply": "2025-06-30T16:35:01.526344Z"
    },
    "papermill": {
     "duration": 0.088009,
     "end_time": "2025-06-30T16:35:01.530789",
     "exception": false,
     "start_time": "2025-06-30T16:35:01.442780",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def impute_missing_data(X_train, X_test, debug=False):\n",
    "    \"\"\"\n",
    "    Impute missing values in training and test datasets based on column types and context.\n",
    "    \n",
    "    Args:\n",
    "        X_train (pd.DataFrame): Training data with features.\n",
    "        X_test (pd.DataFrame): Test data with features.\n",
    "        debug (bool): If True, print debug information (default: False).\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (X_train_imputed, X_test_imputed) - Imputed training and test data.\n",
    "    \"\"\"\n",
    "    X_train_imputed = X_train.copy()\n",
    "    X_test_imputed = X_test.copy()\n",
    "    \n",
    "    # Define imputation strategies\n",
    "    numeric_strategies = {\n",
    "        'median': ['LotFrontage', 'MasVnrArea'],  # Continuous numeric\n",
    "        'ref_col': {'GarageYrBlt': 'YearBuilt'},  # Use value from reference column\n",
    "        'zero': [col for col in X_train.select_dtypes(include=['int64', 'float64']).columns \n",
    "                 if col not in ['LotFrontage', 'MasVnrArea', 'GarageYrBlt']]\n",
    "    }\n",
    "    \n",
    "    categorical_strategies = {\n",
    "        'none': ['BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', \n",
    "                 'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond', 'MasVnrType', \n",
    "                 'FireplaceQu', 'PoolQC', 'Fence', 'Alley', 'MiscFeature'],\n",
    "        'mode': [col for col in X_train.select_dtypes(include=['object']).columns \n",
    "                 if col not in ['BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', \n",
    "                                'BsmtFinType2', 'GarageType', 'GarageFinish', 'GarageQual', \n",
    "                                'GarageCond', 'MasVnrType', 'FireplaceQu', 'PoolQC', 'Fence', \n",
    "                                'Alley', 'MiscFeature']]\n",
    "    }\n",
    "    \n",
    "    # Numeric imputation\n",
    "    # Medians:\n",
    "    for col in numeric_strategies['median']:\n",
    "        median_val = X_train_imputed[col].median()\n",
    "        X_train_imputed[col] = X_train_imputed[col].fillna(median_val)\n",
    "        X_test_imputed[col] = X_test_imputed[col].fillna(median_val)\n",
    "    # Using other Column -- e.g. Year built for Garage Built\n",
    "    for col, ref_col in numeric_strategies['ref_col'].items():\n",
    "        X_train_imputed[col] = X_train_imputed[col].fillna(X_train_imputed[ref_col])\n",
    "        X_test_imputed[col] = X_test_imputed[col].fillna(X_test_imputed[ref_col])\n",
    "    # Fill with Zeros\n",
    "    X_train_imputed[numeric_strategies['zero']] = X_train_imputed[numeric_strategies['zero']].fillna(0)\n",
    "    X_test_imputed[numeric_strategies['zero']] = X_test_imputed[numeric_strategies['zero']].fillna(0)\n",
    "    \n",
    "    # Categorical imputation\n",
    "    for col in categorical_strategies['none']:\n",
    "        X_train_imputed[col] = X_train_imputed[col].fillna('None')\n",
    "        X_test_imputed[col] = X_test_imputed[col].fillna('None')\n",
    "    \n",
    "    for col in categorical_strategies['mode']:\n",
    "        mode_val = X_train_imputed[col].mode()[0]\n",
    "        X_train_imputed[col] = X_train_imputed[col].fillna(mode_val)\n",
    "        X_test_imputed[col] = X_test_imputed[col].fillna(mode_val)\n",
    "    \n",
    "    if debug:\n",
    "        print(\"Missing values after imputation - Train:\", X_train_imputed.isnull().sum().sum())\n",
    "        print(\"Missing values after imputation - Test:\", X_test_imputed.isnull().sum().sum())\n",
    "    \n",
    "    return X_train_imputed, X_test_imputed\n",
    "\n",
    "# Usage\n",
    "X_train_imputed, X_test_imputed = impute_missing_data(X_train, X_test, debug=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457833d0",
   "metadata": {
    "papermill": {
     "duration": 0.005055,
     "end_time": "2025-06-30T16:35:01.543996",
     "exception": false,
     "start_time": "2025-06-30T16:35:01.538941",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8128f93",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-30T16:35:01.557605Z",
     "iopub.status.busy": "2025-06-30T16:35:01.557200Z",
     "iopub.status.idle": "2025-06-30T16:35:01.832468Z",
     "shell.execute_reply": "2025-06-30T16:35:01.831252Z"
    },
    "papermill": {
     "duration": 0.283949,
     "end_time": "2025-06-30T16:35:01.834593",
     "exception": false,
     "start_time": "2025-06-30T16:35:01.550644",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neighborhood stats sample:\n",
      "                  min     max    median  count\n",
      "Neighborhood                                 \n",
      "Blmngtn       159895  264561  191000.0     17\n",
      "Blueste       124000  151000  137500.0      2\n",
      "BrDale         83000  125000  106000.0     16\n",
      "BrkSide        39300  223500  124300.0     58\n",
      "ClearCr       130000  328000  200250.0     28\n",
      "Engineered features summary:\n",
      "           HouseAge       TotalSF    TotalBath     RemodAge    OutdoorSF  \\\n",
      "count  1460.000000   1460.000000  1460.000000  1460.000000  1460.000000   \n",
      "mean     36.547945   3045.873288     2.210616    22.950685   184.088356   \n",
      "std      30.250152    959.534673     0.785399    20.639875   166.418528   \n",
      "min       0.000000    334.000000     1.000000     0.000000     0.000000   \n",
      "25%       8.000000   2396.750000     2.000000     4.000000    45.000000   \n",
      "50%      35.000000   2939.500000     2.000000    14.000000   164.000000   \n",
      "75%      54.000000   3575.750000     2.500000    41.000000   266.250000   \n",
      "max     136.000000  13170.000000     6.000000    60.000000  1560.000000   \n",
      "\n",
      "          QualCond     LotRatio  BsmtFinRatio  GarageScore  \\\n",
      "count  1460.000000  1460.000000   1423.000000  1460.000000   \n",
      "mean     33.864384     0.360627      0.448697     8.392466   \n",
      "std       9.219624     0.204109      0.361698     2.362042   \n",
      "min       1.000000     0.021766      0.000000     0.000000   \n",
      "25%      30.000000     0.252812      0.000000     9.000000   \n",
      "50%      35.000000     0.317368      0.517730     9.000000   \n",
      "75%      40.000000     0.391861      0.770568     9.000000   \n",
      "max      90.000000     1.732308      1.000000    25.000000   \n",
      "\n",
      "       Neighborhood_MedianPrice  Neighborhood_PricePerSqFt  \n",
      "count               1460.000000                1460.000000  \n",
      "mean              174583.954795                 119.330594  \n",
      "std                55900.957072                  18.347056  \n",
      "min                88000.000000                  80.578512  \n",
      "25%               135000.000000                 100.896673  \n",
      "50%               179900.000000                 120.445344  \n",
      "75%               197200.000000                 134.515817  \n",
      "max               315000.000000                 157.957245  \n"
     ]
    }
   ],
   "source": [
    "def engineer_features(train_data, X_test, debug=False):\n",
    "    \"\"\"Engineer additional features for training and test datasets.\n",
    "    \n",
    "    Args:\n",
    "        train_data (pd.DataFrame): Training data with features and target ('SalePrice').\n",
    "        X_test (pd.DataFrame): Test data with features only.\n",
    "        debug (bool): If True, print debug information (default: False).\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (X_train_enriched, X_test_enriched) - Feature-enriched training and test data.\n",
    "    \"\"\"\n",
    "    X_train_enriched = train_data.drop(columns=['SalePrice'], errors='ignore').copy()\n",
    "    X_test_enriched = X_test.copy()\n",
    "    \n",
    "    # 1. Neighborhood Pricing Features\n",
    "    agg_stats = ['min', 'max', 'median', 'count']\n",
    "    neigh_stats = train_data.groupby('Neighborhood', observed=False)['SalePrice'].agg(agg_stats)\n",
    "    for stat in agg_stats:\n",
    "        col_name = f'Neighborhood_{stat.capitalize()}Price'\n",
    "        X_train_enriched[col_name] = X_train_enriched['Neighborhood'].map(neigh_stats[stat])\n",
    "        X_test_enriched[col_name] = X_test_enriched['Neighborhood'].map(neigh_stats[stat])\n",
    "    \n",
    "    neigh_qual_stats = train_data.groupby(['Neighborhood', 'OverallQual'])['SalePrice'].median()\n",
    "    X_train_enriched['Neighborhood_Qual_MedianPrice'] = X_train_enriched.apply(\n",
    "        lambda row: neigh_qual_stats.get((row['Neighborhood'], row['OverallQual']), \n",
    "                                        neigh_stats['median'][row['Neighborhood']]), axis=1\n",
    "    )\n",
    "    X_test_enriched['Neighborhood_Qual_MedianPrice'] = X_test_enriched.apply(\n",
    "        lambda row: neigh_qual_stats.get((row['Neighborhood'], row['OverallQual']), \n",
    "                                        neigh_stats['median'][row['Neighborhood']]), axis=1\n",
    "    )\n",
    "    \n",
    "    train_data['PricePerSqFt'] = train_data['SalePrice'] / train_data['GrLivArea']\n",
    "    neigh_price_sqft = train_data.groupby('Neighborhood')['PricePerSqFt'].median()\n",
    "    X_train_enriched['Neighborhood_PricePerSqFt'] = X_train_enriched['Neighborhood'].map(neigh_price_sqft)\n",
    "    X_test_enriched['Neighborhood_PricePerSqFt'] = X_test_enriched['Neighborhood'].map(neigh_price_sqft)\n",
    "    \n",
    "    # Derived Features\n",
    "    # House Age\n",
    "    X_train_enriched['HouseAge'] = np.maximum(X_train_enriched['YrSold'] - X_train_enriched['YearBuilt'], 0)\n",
    "    X_test_enriched['HouseAge'] = np.maximum(X_test_enriched['YrSold'] - X_test_enriched['YearBuilt'], 0)\n",
    "\n",
    "    # Total SqFt (Living + Basement + Garage)\n",
    "    X_train_enriched['TotalSF'] = (X_train_enriched['GrLivArea'] + X_train_enriched['TotalBsmtSF'] + \n",
    "                                  X_train_enriched['GarageArea'])\n",
    "    X_test_enriched['TotalSF'] = (X_test_enriched['GrLivArea'] + X_test_enriched['TotalBsmtSF'] + \n",
    "                                 X_test_enriched['GarageArea'])\n",
    "\n",
    "    # Total Bathrooms\n",
    "    X_train_enriched['TotalBath'] = (X_train_enriched['FullBath'] + 0.5 * X_train_enriched['HalfBath'] + \n",
    "                                    X_train_enriched['BsmtFullBath'] + 0.5 * X_train_enriched['BsmtHalfBath'])\n",
    "    X_test_enriched['TotalBath'] = (X_test_enriched['FullBath'] + 0.5 * X_test_enriched['HalfBath'] + \n",
    "                                   X_test_enriched['BsmtFullBath'] + 0.5 * X_test_enriched['BsmtHalfBath'])\n",
    "    \n",
    "    # Remodel Age: Years since last remodel (0 if no remodel)\n",
    "    X_train_enriched['RemodAge'] = np.maximum(X_train_enriched['YrSold'] - X_train_enriched['YearRemodAdd'], 0)\n",
    "    X_test_enriched['RemodAge'] = np.maximum(X_test_enriched['YrSold'] - X_test_enriched['YearRemodAdd'], 0)\n",
    "    \n",
    "    # Outdoor Space: Total square footage of outdoor areas\n",
    "    outdoor_cols = ['WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea']\n",
    "    X_train_enriched['OutdoorSF'] = X_train_enriched[outdoor_cols].sum(axis=1)\n",
    "    X_test_enriched['OutdoorSF'] = X_test_enriched[outdoor_cols].sum(axis=1)\n",
    "    \n",
    "    # Quality-Condition Interaction: Product of overall quality and condition\n",
    "    X_train_enriched['QualCond'] = X_train_enriched['OverallQual'] * X_train_enriched['OverallCond']\n",
    "    X_test_enriched['QualCond'] = X_test_enriched['OverallQual'] * X_test_enriched['OverallCond']\n",
    "    \n",
    "    # Lot Utilization: Ratio of total square footage to lot area\n",
    "    X_train_enriched['LotRatio'] = X_train_enriched['TotalSF'] / X_train_enriched['LotArea']\n",
    "    X_test_enriched['LotRatio'] = X_test_enriched['TotalSF'] / X_test_enriched['LotArea']\n",
    "    \n",
    "    # Basement Finish Ratio: Proportion of basement thatâ€™s finished\n",
    "    X_train_enriched['BsmtFinRatio'] = (X_train_enriched['BsmtFinSF1'] + X_train_enriched['BsmtFinSF2']) / \\\n",
    "                                       X_train_enriched['TotalBsmtSF'].replace(0, np.nan).fillna(0)\n",
    "    X_test_enriched['BsmtFinRatio'] = (X_test_enriched['BsmtFinSF1'] + X_test_enriched['BsmtFinSF2']) / \\\n",
    "                                      X_test_enriched['TotalBsmtSF'].replace(0, np.nan).fillna(0)\n",
    "    \n",
    "    # Garage Score: Quality * Condition of garage (numeric mapping)\n",
    "    qual_map = {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'None': 0}\n",
    "    X_train_enriched['GarageScore'] = (X_train_enriched['GarageQual'].map(qual_map, na_action='ignore').fillna(0) * \n",
    "                                      X_train_enriched['GarageCond'].map(qual_map, na_action='ignore').fillna(0))\n",
    "    X_test_enriched['GarageScore'] = (X_test_enriched['GarageQual'].map(qual_map, na_action='ignore').fillna(0) * \n",
    "                                     X_test_enriched['GarageCond'].map(qual_map, na_action='ignore').fillna(0))\n",
    "    \n",
    "    if debug:\n",
    "        print(\"Neighborhood stats sample:\\n\", neigh_stats.head())\n",
    "        print(\"Engineered features summary:\\n\", \n",
    "              X_train_enriched[['HouseAge', 'TotalSF', 'TotalBath', 'RemodAge', 'OutdoorSF', \n",
    "                              'QualCond', 'LotRatio', 'BsmtFinRatio', 'GarageScore', \n",
    "                              'Neighborhood_MedianPrice', 'Neighborhood_PricePerSqFt']].describe())\n",
    "    \n",
    "    return X_train_enriched, X_test_enriched\n",
    "\n",
    "# Usage\n",
    "train_data = X_train_imputed.copy()\n",
    "train_data['SalePrice'] = y_train\n",
    "X_train_enriched, X_test_enriched = engineer_features(train_data, X_test_imputed, debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5812ab59",
   "metadata": {
    "papermill": {
     "duration": 0.006543,
     "end_time": "2025-06-30T16:35:01.849928",
     "exception": false,
     "start_time": "2025-06-30T16:35:01.843385",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## One-Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c35b2d2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-30T16:35:01.861937Z",
     "iopub.status.busy": "2025-06-30T16:35:01.861437Z",
     "iopub.status.idle": "2025-06-30T16:35:02.707636Z",
     "shell.execute_reply": "2025-06-30T16:35:02.706217Z"
    },
    "papermill": {
     "duration": 0.855104,
     "end_time": "2025-06-30T16:35:02.710556",
     "exception": false,
     "start_time": "2025-06-30T16:35:01.855452",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape after encoding: (1460, 232)\n",
      "Test shape after encoding: (1459, 232)\n",
      "Original categorical columns: 43\n",
      "Encoded columns added: 181\n",
      "Sample categorical columns: ['MSZoning', 'Street', 'Alley', 'LotShape', 'LandContour']\n"
     ]
    }
   ],
   "source": [
    "def one_hot_encode_data(X_train, X_test, min_freq=0.01, debug=False):\n",
    "    \"\"\"Perform one-hot encoding on training and test datasets, dropping rare categories.\n",
    "    \n",
    "    Args:\n",
    "        X_train (pd.DataFrame): Training data with features.\n",
    "        X_test (pd.DataFrame): Test data with features.\n",
    "        min_freq (float): Minimum frequency threshold for categories to keep (default: 0.01, (1%)).\n",
    "        debug (bool): If True, print debug information (default: False).\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (X_train_oh, X_test_oh) - One-hot encoded training and test data.\n",
    "    \"\"\"\n",
    "    categorical_cols = X_train.select_dtypes(include=['object']).columns.tolist()\n",
    "    \n",
    "    # One-hot encode with frequency filtering\n",
    "    X_train_oh = pd.get_dummies(X_train, columns=categorical_cols, sparse=False, dtype=int)\n",
    "    X_test_oh = pd.get_dummies(X_test, columns=categorical_cols, sparse=False, dtype=int)\n",
    "    \n",
    "    # Filter rare categories (less than min_freq in training data)\n",
    "    for col in X_train_oh.columns:\n",
    "        if col not in X_train.select_dtypes(exclude=['object']).columns:  # Only encoded cols\n",
    "            freq = X_train_oh[col].mean()  # Proportion of 1s\n",
    "            if freq < min_freq:\n",
    "                X_train_oh.drop(col, axis=1, inplace=True)\n",
    "                if col in X_test_oh.columns:\n",
    "                    X_test_oh.drop(col, axis=1, inplace=True)\n",
    "    \n",
    "    # Align train and test columns\n",
    "    X_train_oh, X_test_oh = X_train_oh.align(X_test_oh, join='outer', axis=1, fill_value=0)\n",
    "    \n",
    "    if debug:\n",
    "        original_cols = X_train.shape[1]\n",
    "        encoded_cols = X_train_oh.shape[1] - X_train.select_dtypes(exclude=['object']).columns.size\n",
    "        print(f\"Train shape after encoding: {X_train_oh.shape}\")\n",
    "        print(f\"Test shape after encoding: {X_test_oh.shape}\")\n",
    "        print(f\"Original categorical columns: {len(categorical_cols)}\")\n",
    "        print(f\"Encoded columns added: {encoded_cols}\")\n",
    "        print(f\"Sample categorical columns: {categorical_cols[:5]}\")\n",
    "    \n",
    "    return X_train_oh, X_test_oh\n",
    "\n",
    "# Usage\n",
    "X_train_oh, X_test_oh = one_hot_encode_data(X_train_enriched, X_test_enriched, min_freq=0.01, debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfc8a7e",
   "metadata": {
    "papermill": {
     "duration": 0.005224,
     "end_time": "2025-06-30T16:35:02.723021",
     "exception": false,
     "start_time": "2025-06-30T16:35:02.717797",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Train / Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54fe2a77",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-30T16:35:02.734569Z",
     "iopub.status.busy": "2025-06-30T16:35:02.734194Z",
     "iopub.status.idle": "2025-06-30T16:35:02.745619Z",
     "shell.execute_reply": "2025-06-30T16:35:02.744281Z"
    },
    "papermill": {
     "duration": 0.019207,
     "end_time": "2025-06-30T16:35:02.747588",
     "exception": false,
     "start_time": "2025-06-30T16:35:02.728381",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def split_train_validation(X, y, train_size=0.8, random_state=0, debug=False):\n",
    "    \"\"\"\n",
    "    Split data into training and validation sets.\n",
    "    \n",
    "    Args:\n",
    "        X (pd.DataFrame): Feature data to split.\n",
    "        y (pd.Series): Target data to split.\n",
    "        train_size (float): Proportion of data for training (default: 0.8).\n",
    "        random_state (int): Seed for reproducibility (default: 0).\n",
    "        debug (bool): If True, print debug information (default: False).\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (X_train, X_valid, y_train, y_valid) - Split training and validation data.\n",
    "    \"\"\"\n",
    "    # Ensure target aligns with features\n",
    "    y = y.reindex(X.index)\n",
    "    \n",
    "    # Split into training and validation sets\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "        X, y, train_size=train_size, test_size=1 - train_size, random_state=random_state\n",
    "    )\n",
    "    \n",
    "    if debug:\n",
    "        print(\"Train split shape:\", X_train.shape)\n",
    "        print(\"Validation split shape:\", X_valid.shape)\n",
    "    \n",
    "    return X_train, X_valid, y_train, y_valid\n",
    "\n",
    "# Usage\n",
    "X_train_split, X_valid_split, y_train_split, y_valid_split = split_train_validation(\n",
    "    X_train_oh, y_train, debug=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98fd7558",
   "metadata": {
    "papermill": {
     "duration": 0.004513,
     "end_time": "2025-06-30T16:35:02.757139",
     "exception": false,
     "start_time": "2025-06-30T16:35:02.752626",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20e09700",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-30T16:35:02.768115Z",
     "iopub.status.busy": "2025-06-30T16:35:02.767767Z",
     "iopub.status.idle": "2025-06-30T16:35:07.902926Z",
     "shell.execute_reply": "2025-06-30T16:35:07.901282Z"
    },
    "papermill": {
     "duration": 5.143528,
     "end_time": "2025-06-30T16:35:07.905522",
     "exception": false,
     "start_time": "2025-06-30T16:35:02.761994",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation MAE: 16472.3345\n",
      "5-Fold CV MAE: 16508.0094 Â± 1402.9419\n",
      "Best iteration: 88\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np\n",
    "\n",
    "def evaluate_model(X_train, y_train, X_valid, y_valid, cv_folds=5, random_state=0, debug=False):\n",
    "    \"\"\"\n",
    "    Evaluate an XGBoost model with cross-validation and validation set metrics.\n",
    "    \n",
    "    Args:\n",
    "        X_train (pd.DataFrame): Training features.\n",
    "        y_train (pd.Series): Training target.\n",
    "        X_valid (pd.DataFrame): Validation features.\n",
    "        y_valid (pd.Series): Validation target.\n",
    "        cv_folds (int): Number of cross-validation folds (default: 5).\n",
    "        random_state (int): Seed for reproducibility (default: 0).\n",
    "        debug (bool): If True, print debug information (default: False).\n",
    "    \n",
    "    Returns:\n",
    "        dict: Evaluation metrics (RMSE, MAE, CV MAE mean/std), and the fitted model.\n",
    "    \"\"\"\n",
    "\n",
    "    # Model with early stopping for validation set evaluation\n",
    "    model_with_es = XGBRegressor(\n",
    "        n_estimators=1000, # High value since early stopping will optimize\n",
    "        random_state=random_state, \n",
    "        objective='reg:squarederror',\n",
    "        eval_metric=EVAL_METRIC, \n",
    "        early_stopping_rounds=10, \n",
    "        learning_rate=0.05\n",
    "    )\n",
    "\n",
    "    # Fit model with early stopping using validation set\n",
    "    model_with_es.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], verbose=False)\n",
    "\n",
    "    # Validation set predictions and metrics\n",
    "    y_pred = model_with_es.predict(X_valid)\n",
    "\n",
    "    # Map eval_metric to scoring function\n",
    "    metric_func, cv_scoring, metric_name = METRIC_MAP.get(EVAL_METRIC, METRIC_MAP['mae'])  # Default to MAE\n",
    "    valid_metric = metric_func(y_valid, y_pred)\n",
    "\n",
    "    # Required for CV scoring\n",
    "    model_no_es = XGBRegressor(\n",
    "        n_estimators=model_with_es.best_iteration, \n",
    "        random_state=random_state,\n",
    "        objective='reg:squarederror', \n",
    "        eval_metric=EVAL_METRIC\n",
    "    )\n",
    "    cv_scores = cross_val_score(model_no_es, X_train, y_train, cv=cv_folds, scoring=cv_scoring, n_jobs=-1)\n",
    "    metrics = {\n",
    "        'valid_metric': valid_metric,\n",
    "        'cv_metric_mean': -cv_scores.mean(),\n",
    "        'cv_metric_std': cv_scores.std()\n",
    "    }\n",
    "\n",
    "    # Calculate translated metric in original price scale if log transform is on\n",
    "    if USE_LOG_TRANSFORM and debug:\n",
    "        y_valid_orig = np.expm1(y_valid)\n",
    "        y_pred_orig = np.expm1(y_pred)\n",
    "        if EVAL_METRIC == 'mae':\n",
    "            translated_metric = mean_absolute_error(y_valid_orig, y_pred_orig)\n",
    "            metrics['translated_mae'] = translated_metric\n",
    "        elif EVAL_METRIC == 'rmse':\n",
    "            translated_metric = mean_squared_error(y_valid_orig, y_pred_orig, squared=False)\n",
    "            metrics['translated_rmse'] = translated_metric\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"Validation {metric_name}: {valid_metric:.4f}{' (log scale)' if USE_LOG_TRANSFORM else ''}\")\n",
    "        print(f\"{cv_folds}-Fold CV {metric_name}: {metrics['cv_metric_mean']:.4f} Â± {metrics['cv_metric_std']:.4f}{' (log scale)' if USE_LOG_TRANSFORM else ''}\")\n",
    "        if USE_LOG_TRANSFORM and EVAL_METRIC in ['mae', 'rmse']:\n",
    "            print(f\"Translated Validation {metric_name}: {translated_metric:.2f}\")\n",
    "        print(f\"Best iteration: {model_with_es.best_iteration}\")\n",
    "    return metrics, model_with_es\n",
    "\n",
    "# Usage\n",
    "metrics, baseline_model = evaluate_model(\n",
    "    X_train_split, y_train_split, X_valid_split, y_valid_split, debug=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dadbddb",
   "metadata": {
    "papermill": {
     "duration": 0.00511,
     "end_time": "2025-06-30T16:35:07.915808",
     "exception": false,
     "start_time": "2025-06-30T16:35:07.910698",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Drop Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50820fd4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-30T16:35:07.927559Z",
     "iopub.status.busy": "2025-06-30T16:35:07.927125Z",
     "iopub.status.idle": "2025-06-30T16:35:07.958038Z",
     "shell.execute_reply": "2025-06-30T16:35:07.956855Z"
    },
    "papermill": {
     "duration": 0.038746,
     "end_time": "2025-06-30T16:35:07.959929",
     "exception": false,
     "start_time": "2025-06-30T16:35:07.921183",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used threshold: 0.00010\n",
      "Dropped 81 features out of 232\n",
      "Remaining features: 151 (Retention: 65.09%)\n",
      "Top 10 feature importances:\n",
      " Neighborhood_Qual_MedianPrice    0.432691\n",
      "TotalSF                          0.080904\n",
      "BsmtQual_Ex                      0.029020\n",
      "KitchenQual_TA                   0.023776\n",
      "Fence_GdPrv                      0.017003\n",
      "CentralAir_N                     0.012003\n",
      "KitchenQual_Gd                   0.009883\n",
      "RemodAge                         0.009352\n",
      "Exterior2nd_HdBoard              0.009247\n",
      "TotalBath                        0.008582\n",
      "dtype: float32\n",
      "\n",
      "Threshold vs. Features Kept (sample):\n",
      "Threshold 0.00010: 151 features (65.09% retention)\n",
      "Threshold 0.00210: 73 features (31.47% retention)\n",
      "Threshold 0.00410: 41 features (17.67% retention)\n",
      "Threshold 0.00610: 20 features (8.62% retention)\n",
      "Threshold 0.00810: 12 features (5.17% retention)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def drop_features(X_train, X_valid, X_test, model, threshold=None, target_retention=0.6, use_known_threshold=False, debug=False):\n",
    "    \"\"\"Drop low-importance features based on a threshold or automatically determine it.\n",
    "    \n",
    "    Args:\n",
    "        X_train (pd.DataFrame): Training features.\n",
    "        X_valid (pd.DataFrame): Validation features.\n",
    "        X_test (pd.DataFrame): Test features.\n",
    "        model (XGBRegressor): Fitted model with feature importances.\n",
    "        threshold (float, optional): Feature importance threshold to drop below. If None, auto-determine.\n",
    "        target_retention (float): Target fraction of features to retain (default: 0.6, i.e., 60%).\n",
    "        use_known_threshold (bool): If True, use 0.00055 instead of searching (default: False).\n",
    "        debug (bool): If True, print debug information and optionally plot threshold vs. features (default: False).\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (X_train_reduced, X_valid_reduced, X_test_reduced, used_threshold) - Reduced datasets and threshold used.\n",
    "    \"\"\"\n",
    "    importances = pd.Series(model.feature_importances_, index=X_train.columns)\n",
    "    \n",
    "    if threshold is not None:\n",
    "        used_threshold = threshold\n",
    "    elif use_known_threshold:\n",
    "        used_threshold = 0.00055  # Known good value from previous runs\n",
    "    else:\n",
    "        # Automatic threshold search\n",
    "        thresholds = np.linspace(0.0001, 0.01, 100)\n",
    "        feature_counts = [sum(importances >= t) for t in thresholds]\n",
    "        target_count = int(target_retention * len(importances))\n",
    "        used_threshold = thresholds[np.argmin([abs(count - target_count) for count in feature_counts])]\n",
    "    \n",
    "    # Drop low-importance features\n",
    "    low_imp_features = importances[importances < used_threshold].index.tolist()\n",
    "    X_train_reduced = X_train.drop(columns=low_imp_features)\n",
    "    X_valid_reduced = X_valid.drop(columns=low_imp_features)\n",
    "    X_test_reduced = X_test.drop(columns=low_imp_features)\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"Used threshold: {used_threshold:.5f}\")\n",
    "        print(f\"Dropped {len(low_imp_features)} features out of {len(importances)}\")\n",
    "        print(f\"Remaining features: {X_train_reduced.shape[1]} (Retention: {X_train_reduced.shape[1]/len(importances):.2%})\")\n",
    "        print(\"Top 10 feature importances:\\n\", importances.nlargest(10))\n",
    "        if threshold is None and not use_known_threshold:\n",
    "            # Extended debug info for threshold search\n",
    "            print(\"\\nThreshold vs. Features Kept (sample):\")\n",
    "            for t, count in list(zip(thresholds, feature_counts))[::20]:  # Show every 20th for brevity\n",
    "                print(f\"Threshold {t:.5f}: {count} features ({count/len(importances):.2%} retention)\")\n",
    "    \n",
    "    return X_train_reduced, X_valid_reduced, X_test_reduced, used_threshold\n",
    "\n",
    "# Usage\n",
    "X_train_red, X_valid_red, X_test_red, thresh = drop_features(\n",
    "    X_train_split, X_valid_split, X_test_oh, baseline_model, \n",
    "    threshold=None, target_retention=0.8, use_known_threshold=False, debug=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7214bc60",
   "metadata": {
    "papermill": {
     "duration": 0.004574,
     "end_time": "2025-06-30T16:35:07.969479",
     "exception": false,
     "start_time": "2025-06-30T16:35:07.964905",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Tune Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30a59134",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-30T16:35:07.980667Z",
     "iopub.status.busy": "2025-06-30T16:35:07.980298Z",
     "iopub.status.idle": "2025-06-30T16:41:12.209741Z",
     "shell.execute_reply": "2025-06-30T16:41:12.208356Z"
    },
    "papermill": {
     "duration": 364.242375,
     "end_time": "2025-06-30T16:41:12.216682",
     "exception": false,
     "start_time": "2025-06-30T16:35:07.974307",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'subsample': 0.8, 'min_child_weight': 1, 'max_depth': 5, 'learning_rate': 0.025, 'gamma': 0.1, 'colsample_bytree': 0.7}\n",
      "Best CV MAE: 14450.3097\n",
      "Validation MAE: 15343.2544\n",
      "Best iteration: 348\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "def tune_model(X_train, y_train, X_valid, y_valid, random_state=0, debug=False):\n",
    "    \"\"\"Tune an XGBoost model using RandomizedSearchCV on a reduced feature set.\n",
    "    \n",
    "    Args:\n",
    "        X_train (pd.DataFrame): Training features (reduced).\n",
    "        y_train (pd.Series): Training target.\n",
    "        X_valid (pd.DataFrame): Validation features (reduced).\n",
    "        y_valid (pd.Series): Validation target.\n",
    "        random_state (int): Seed for reproducibility (default: 0).\n",
    "        debug (bool): If True, print debug information (default: False).\n",
    "    \n",
    "    Returns:\n",
    "        XGBRegressor: Best tuned model.\n",
    "    \"\"\"\n",
    "    metric_func, cv_scoring, metric_name = METRIC_MAP.get(EVAL_METRIC, METRIC_MAP['mae'])\n",
    "    scoring = cv_scoring\n",
    "    \n",
    "    base_model = XGBRegressor(\n",
    "        n_estimators=1000, objective='reg:squarederror', eval_metric=EVAL_METRIC,\n",
    "        early_stopping_rounds=10, random_state=random_state \n",
    "        # nthread=1 # Force single-threaded XGBoost\n",
    "    )\n",
    "    \n",
    "    # Expanded parameter grid\n",
    "    param_dist = {\n",
    "        'learning_rate': [0.01, 0.025, 0.05, 0.1],\n",
    "        'max_depth': [3, 5, 7, 9],\n",
    "        'subsample': [0.7, 0.8, 0.9, 1.0],\n",
    "        'colsample_bytree': [0.7, 0.8, 0.9, 1.0],\n",
    "        'min_child_weight': [1, 3, 5], \n",
    "        'gamma': [0, 0.1, 0.5, 1] \n",
    "    }\n",
    "    \n",
    "    random_search = RandomizedSearchCV(\n",
    "        base_model, param_dist, n_iter=50, cv=5, scoring=scoring,\n",
    "        random_state=random_state, n_jobs=2, verbose=0\n",
    "    )\n",
    "    random_search.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], verbose=False)\n",
    "    best_model = random_search.best_estimator_\n",
    "    \n",
    "    y_pred = best_model.predict(X_valid)\n",
    "    valid_metric = metric_func(y_valid, y_pred)\n",
    "    \n",
    "    if USE_LOG_TRANSFORM and debug:\n",
    "        y_valid_orig = np.expm1(y_valid)\n",
    "        y_pred_orig = np.expm1(y_pred)\n",
    "        if EVAL_METRIC == 'mae':\n",
    "            translated_metric = mean_absolute_error(y_valid_orig, y_pred_orig)\n",
    "        elif EVAL_METRIC == 'rmse':\n",
    "            translated_metric = mean_squared_error(y_valid_orig, y_pred_orig, squared=False)\n",
    "    \n",
    "    if debug:\n",
    "        print(\"Best parameters:\", random_search.best_params_)\n",
    "        print(f\"Best CV {metric_name}: {-random_search.best_score_:.4f}{' (log scale)' if USE_LOG_TRANSFORM else ''}\")\n",
    "        print(f\"Validation {metric_name}: {valid_metric:.4f}{' (log scale)' if USE_LOG_TRANSFORM else ''}\")\n",
    "        if USE_LOG_TRANSFORM and EVAL_METRIC in ['mae', 'rmse']:\n",
    "            print(f\"Translated Validation {metric_name}: {translated_metric:.2f}\")\n",
    "        print(f\"Best iteration: {best_model.best_iteration}\")\n",
    "    \n",
    "    return best_model\n",
    "\n",
    "# Usage\n",
    "tuned_model = tune_model(X_train_red, y_train_split, X_valid_red, y_valid_split, debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4dfd3c1",
   "metadata": {
    "papermill": {
     "duration": 0.005336,
     "end_time": "2025-06-30T16:41:12.227374",
     "exception": false,
     "start_time": "2025-06-30T16:41:12.222038",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63779240",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-30T16:41:12.239071Z",
     "iopub.status.busy": "2025-06-30T16:41:12.238656Z",
     "iopub.status.idle": "2025-06-30T16:41:13.389964Z",
     "shell.execute_reply": "2025-06-30T16:41:13.388552Z"
    },
    "papermill": {
     "duration": 1.15968,
     "end_time": "2025-06-30T16:41:13.392040",
     "exception": false,
     "start_time": "2025-06-30T16:41:12.232360",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission saved to submission.csv\n",
      "Log transform: Off\n",
      "Sample predictions:\n",
      "      Id      SalePrice\n",
      "0  1461  133087.984375\n",
      "1  1462  159701.000000\n",
      "2  1463  181526.093750\n",
      "3  1464  189335.625000\n",
      "4  1465  180889.343750\n",
      "Test prediction summary:\n",
      " count      1459.00\n",
      "mean     178728.45\n",
      "std       76656.42\n",
      "min       48996.82\n",
      "25%      128467.24\n",
      "50%      158386.70\n",
      "75%      208587.18\n",
      "max      566388.50\n",
      "Name: SalePrice, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def generate_submission(X_train, y_train, X_test, model, output_file='submission.csv', debug=False):\n",
    "    \"\"\"Fit final model and generate test predictions, reversing log transform if enabled.\n",
    "    \n",
    "    Args:\n",
    "        X_train (pd.DataFrame): Full training features (reduced).\n",
    "        y_train (pd.Series): Full training target.\n",
    "        X_test (pd.DataFrame): Test features (reduced).\n",
    "        model (XGBRegressor): Tuned model with best parameters.\n",
    "        output_file (str): Path to save the submission CSV (default: 'submission.csv').\n",
    "        debug (bool): If True, print debug information (default: False).\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Submission DataFrame with Id and SalePrice.\n",
    "    \"\"\"\n",
    "    final_model = XGBRegressor(\n",
    "        n_estimators=model.best_iteration, learning_rate=model.learning_rate,\n",
    "        max_depth=model.max_depth, subsample=model.subsample,\n",
    "        colsample_bytree=model.colsample_bytree, objective='reg:squarederror',\n",
    "        eval_metric=EVAL_METRIC, random_state=0\n",
    "    )\n",
    "    \n",
    "    final_model.fit(X_train, y_train, verbose=False)\n",
    "    test_preds = final_model.predict(X_test)\n",
    "    if USE_LOG_TRANSFORM:\n",
    "        test_preds = np.expm1(test_preds)  # Reverse log transform if enabled\n",
    "    \n",
    "    submission = pd.DataFrame({\n",
    "        'Id': X_test.index,\n",
    "        'SalePrice': test_preds\n",
    "    })\n",
    "    submission.to_csv(output_file, index=False)\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"Submission saved to {output_file}\")\n",
    "        print(f\"Log transform: {'On' if USE_LOG_TRANSFORM else 'Off'}\")\n",
    "        print(\"Sample predictions:\\n\", submission.head())\n",
    "        print(\"Test prediction summary:\\n\", pd.Series(test_preds, name='SalePrice').describe().round(2))\n",
    "    \n",
    "    return submission\n",
    "\n",
    "# Usage\n",
    "X_full_red = pd.concat([X_train_red, X_valid_red])\n",
    "y_full = pd.concat([y_train_split, y_valid_split])\n",
    "submission = generate_submission(X_full_red, y_full, X_test_red, tuned_model, debug=True)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 111096,
     "sourceId": 10211,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 379.923367,
   "end_time": "2025-06-30T16:41:14.224874",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-06-30T16:34:54.301507",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
